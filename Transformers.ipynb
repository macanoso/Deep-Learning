{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Workshop 7"
      ],
      "metadata": {
        "id": "Exu-uLmJvAGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Mateo Cano Solís - 1037660293\n",
        "* Keyla García Jaimes - 1026159841"
      ],
      "metadata": {
        "id": "Q79PNycTvJU_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q4I1LSDsu-uZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Descargar y explorar Stack Overflow dataset\n",
        "Descarguemos y extraigamos el conjunto de datos, luego exploremos la estructura del directorio."
      ],
      "metadata": {
        "id": "s9s-JUIRvxtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and exploring the StackOverflow dataset\n",
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "dataset = tf.keras.utils.get_file(\n",
        "  'stack_overflow_16k.tar.gz',\n",
        "  data_url,\n",
        "  untar=True,  \n",
        "  cache_dir='stack_overflow',\n",
        "  cache_subdir='.')\n",
        "dataset_dir = pathlib.Path(dataset).parent"
      ],
      "metadata": {
        "id": "qSuRke3zvg-i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimamos un archivo de ejemplo"
      ],
      "metadata": {
        "id": "0EM-gbYu4-IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "sample_file = os.path.join(train_dir, 'python/1755.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2wu-qN0vj4q",
        "outputId": "c84fed03-8bf5-4c26-f0ce-a2bf31aaab4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why does this blank program print true x=true.def stupid():.    x=false.stupid().print x\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dentro de la carpeta esta el set de entrenamiento y el de testeo"
      ],
      "metadata": {
        "id": "1a8leZr95HLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgX6aKgYvmya",
        "outputId": "82467de3-6e1d-4967-cfff-87a359ffccf1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stack_overflow_16k.tar.gz', 'README.md', 'test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dentro del de entranamiento están los siguientes conjuntos de programas, que permiten etiquetar cada una de las preguntas"
      ],
      "metadata": {
        "id": "l2bkofx_5MFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44DltkKjvpam",
        "outputId": "61c605d4-f624-40a9-c3cc-78f03426e0e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['csharp', 'java', 'python', 'javascript']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cargar Dataset\n",
        "No es necesario remover carpetas ya que no se cuenta con carpetas adicionales a las clases necesarias. \n",
        "Al ejecutar un experimento de aprendizaje automático, se recomienda dividir el conjunto de datos en tres divisiones: entrenar, [validación](https:/ /developers.google.com/machine-learning/glossary#validation_set) y test. El conjunto de datos de Stack OverFlow ya se ha dividido en entrenamiento y prueba, pero carece de un conjunto de validación. Vamos a crear un conjunto de validación usando una división 80:20 de los datos de entrenamiento usando el argumento validation_split a continuación."
      ],
      "metadata": {
        "id": "1-BgtNRfv9XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "  train_dir,\n",
        "  batch_size=batch_size,\n",
        "  validation_split=0.2,\n",
        "  subset='training',\n",
        "  seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQL6OnGxgM0",
        "outputId": "7306696d-610a-4ff6-f3f5-e33f0d71fb6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay 8000 ejemplos en la carpeta de capacitación, de los cuales usará el 80 % (o 6400) para training.\n",
        "A continuación, se creará un conjunto de datos de validación y prueba. Utilizará las 1600 revisiones restantes del conjunto de capacitación para la validación"
      ],
      "metadata": {
        "id": "5V_bZ1wFyQGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    train_dir, \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsfAJxZ8zQ2t",
        "outputId": "f758ce25-aa81-4164-ceb1-4ba4e6e6e7ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    train_dir, \n",
        "    batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCoOT8f6zaDw",
        "outputId": "f1e2af46-919f-4df9-fd0a-ec870a4229fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 files belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cljnmI4Dz96E",
        "outputId": "5b558eda-23ad-4b5c-9a9a-8d29071a661f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to csharp\n",
            "Label 1 corresponds to java\n",
            "Label 2 corresponds to javascript\n",
            "Label 3 corresponds to python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparar el dataset para el entrenamiento\n",
        "* Estandarización\n",
        "* Tokenización\n",
        "* Vectorización"
      ],
      "metadata": {
        "id": "KKxr9_gyzhax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return lowercase"
      ],
      "metadata": {
        "id": "9itqk19T0IPK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "metadata": {
        "id": "CZxbarvK2evX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, llamará a `adapt` para ajustar el estado de la capa de preprocesamiento al conjunto de datos. Esto hará que el modelo genere un índice de cadenas a números enteros."
      ],
      "metadata": {
        "id": "Y72C_I1o27SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "metadata": {
        "id": "Pv-qSGA323XS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear una función para ver el resultado de usar esta capa para preprocesar algunos datos."
      ],
      "metadata": {
        "id": "vOQYmpVr3hdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "jsKqc0z_3iZy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QSpjrXU3rMB",
        "outputId": "b0f19a8b-d787-427a-c575-0b653c7b9a5b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review tf.Tensor(b'\"unit testing of setters and getters teacher wanted us to do a comprehensive unit test. for me, this will be the first time that i use junit. i am confused about testing set and get methods. do you think should i test them? if the answer is yes; is this code enough for testing?..  public void testsetandget(){.    int a = 10;.    class firstclass = new class();.    firstclass.setvalue(10);.    int value = firstclass.getvalue();.    assert.asserttrue(\"\"error\"\", value==a);.  }...in my code, i think if there is an error, we can\\'t know that the error is deriving because of setter or getter.\"\\n', shape=(), dtype=string)\n",
            "Label java\n",
            "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[   1,  908,   11, 3870,   10, 2472, 3454,  708, 1636,    4,   43,\n",
            "           6,    1, 1395, 2618,   13, 1538,   16,   69,   32,    2,  105,\n",
            "         127,   15,    5,   70,    1,    5,   33, 1061,  193,  908,  156,\n",
            "          10,   48, 1912,   43,   65,  301,   92,    5,  264, 6592,   12,\n",
            "           2,  200,    8,    1,    8,   16,   37, 1437,   13,    1,   28,\n",
            "          42,    1,   40,    6,    3, 1026,   34,    1,    3,   18,    1,\n",
            "           1,   40,   72,    3,    1,    1,    1, 4804,   24,  412,    5,\n",
            "         301,   12,   68,    8,   30, 1090,  168,  184,  101,   15,    2,\n",
            "          95,    8,    1,  187,   11, 3651,   46,    1,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos la representación numerica que palabras posee\n"
      ],
      "metadata": {
        "id": "gxWwOXZQ5q5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0FaqmJb4N5a",
        "outputId": "e7121344-04a1-4066-812e-465c779b9b77"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1287 --->  direction\n",
            " 313 --->  instead\n",
            "Vocabulary size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se aplicará la capa TextVectorization al conjunto de datos de entrenamiento, validación y prueba."
      ],
      "metadata": {
        "id": "yG-ElyEM4VAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "nkNNL9Ec4Yvg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "F7kGKn8H4lrl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear el modelo "
      ],
      "metadata": {
        "id": "btAm9yyK4y32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "TiZz64Ox41D1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "Ph2SePNm47-g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "0gbQR7-e5H9t"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000 \n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "maxlen = 250\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "UWqWmo015ZX7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la 3era epoca el modelo alcanzo un acuracy mayor al 80% además de no presentar overfitting, para mas epocas el modelo comienza a aprenderse los ejemplos"
      ],
      "metadata": {
        "id": "zhY2Fq6f8bMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss= losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(\n",
        "    train_ds, batch_size=32, epochs=5, validation_data=val_ds\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0bRVC9n5ieO",
        "outputId": "c138b442-80aa-431c-c657-b5597d03852e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200/200 [==============================] - 39s 185ms/step - loss: 1.3046 - accuracy: 0.3483 - val_loss: 1.0961 - val_accuracy: 0.5344\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 35s 174ms/step - loss: 0.8371 - accuracy: 0.6519 - val_loss: 0.5376 - val_accuracy: 0.8138\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 36s 179ms/step - loss: 0.4165 - accuracy: 0.8413 - val_loss: 0.5270 - val_accuracy: 0.8112\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 34s 172ms/step - loss: 0.2246 - accuracy: 0.9255 - val_loss: 0.7214 - val_accuracy: 0.8056\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 35s 173ms/step - loss: 0.1236 - accuracy: 0.9631 - val_loss: 0.8274 - val_accuracy: 0.7931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el set de testeo tenemos un acuracy que llega al 95%, lo que muestra la gran precision del modelo"
      ],
      "metadata": {
        "id": "t7jfpM0t8rLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds,return_dict = True)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVJjhGmo5iTB",
        "outputId": "07cfd9fe-5c24-4676-cf10-41f412ea517d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 20s 81ms/step - loss: 0.2054 - accuracy: 0.9469\n",
            "Loss:  loss\n",
            "Accuracy:  accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por tal motivo se muestra el gran trabajo del transformer para generalizar con problemas multiclases además de brindar accuracy tan altas. También la gran ventaja al ser modelos más livianos y en teoria con una arquitectura mas liviana. "
      ],
      "metadata": {
        "id": "1AUr3po69f3i"
      }
    }
  ]
}